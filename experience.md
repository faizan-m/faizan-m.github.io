---
layout: article
title: Experience
key: page-experience
aside:
  toc: true
---

## Software Engineering Intern, CTRL Labs
#### (C++, Python, Go)
### May 2019 - August 2019

CTRL Labs is developing an EMG-based, non-invasive neural interface called CTRL Kit with potential applications in AR/VR, Robotics and general Human-Machine Interaction. 

My first project was interfacing a hexapod robot with CTRL Kit allowing the robot's legs to mimic finger movements of a human user. I also developed a "soccer-mode" that allows the user to move the robot and individually kick its legs using muscle movements and gestures. I believe such control schemes have the potential to be incredibly intuitive and can help break down the interaction barrier with robots. 

For my main project, I proposed some experimental features for CTRL Kit involving physically contextualized interactions with real-world objects and spaces. I built a prototype for those features as a proof of concept. Four patent applications are currently being filed based on the proposal idea and prototype.

The hexapod was featured in an NPR video recently:
<div>{%- include extensions/youtube.html id='cdZLg4IORc0' -%}</div>

## Research Assistant, Autonomous Intelligent Robotics Lab, Tufts University 
#### (ROS, Unity, C++, C#, Python)
### Jan 2018 - Present

I started working with Professor Jivko Sinapov starting Spring semester of my Sophomore year. During the first few months, I developed some software in ROS for data-logging the Turtlebot robots in our lab. 

I noticed that learning to work with the robots as a beginner was incredibly difficult and it was next to impossible sometimes to figure what was going on inside them when something went wrong. So for Summer 2018, I proposed an Augmented Reality interface for these Turtlebots that would allow users to visualize the robots' state. This proposal was accepted and funded by Tufts Summer Scholars. The project later grew to include multiple undergraduate students and one graduate student. 

The interface allows a user to visualize a robot's perception, knowledge and intent as an additional visual layer over the real world thereby bridging the gap between the digital information inside the robot and our analogue way of interpretation. 

My role in this project was designing the overall system architecture and then creating a pipeline for transforming and transporting robot data from ROS to Unity so that it could be used to create any kind of visualizations. The interface supports multiple devices including Android smartphones, iOS devices, and Microsoft Hololens. 

We have conducted a preliminary pilot study and are now designing a full-scale study investigating the potential of this interface as a tool for Human-Robot Interaction.

This project has received a lot of attention within the university and beyond:
- I presented a [Late Breaking Report](https://ieeexplore.ieee.org/document/8673191) in HRI 2019 conference in South Korea. The paper contains details about our system architecture and implementation.
- It was part of the Tufts [proposal](https://www.eecs.tufts.edu/~jsinapov/VAR5G/) that won the 2019 [Verizon 5G EdTech Challenge](https://www.5gedtechchallenge.com/)
- It has been featured in an official Tufts University [video](https://www.youtube.com/watch?v=9_9RNRNd9y8) and an [article](https://now.tufts.edu/articles/hands-research-undergraduates)


<div>{%- include extensions/youtube.html id='WjxJnggaNr8' -%}</div>

## Co-President, Tufts Robotics Club
### Apr 2018 - Apr 2019

<div class="hero hero--dark" style='height: 500px; background-image: url("/assets/images/experience/club.JPG");'>
</div>

I have been a part of the Tufts Robotics Club since freshman year and have been particularly involved with the [development of fire-fighting robots](/projects.html#trinity-college-international-fire-fighting-robot-contest). 

During the sophomore year, I was a Software Specialist in the Executive Board and focused on designing software architecture for our projects and teaching new members about software development for robots. 

In my junior year, I was elected Co-President of the club. My main objective in that role personally was improving club diversity, accessibility and member retention. These issues were approached by formalizing member attendance during meetings to keep track of member activity and club growth. It was discovered that new members felt like they lacked ownership in the seemingly complex projects the club was working on. This gave rise to a sense that they could not contribute much to the club. Tackling this problem involved breaking down key project goals with them and identifying tasks that they could do or learn to do. 

Based on the advice provided by some of the Tufts' researchers working on these issues, club dynamics for members were reshaped to ensure that the space was comfortable and welcoming for all skill levels. Active members doubled during the year which later created the most diverse Executive Board in club history.


## Research Assistant, Center for Engineering Education and Outreach, Tufts University
#### (OpenCV, C++, LabVIEW, Arduino)
### Dec 2016 - Aug 2017
I worked in Dr. Ethan Danahy's Lab in CEEO during the freshman year of college focusing on some of the ongoing lab and research projects. My first project was designing a RESTful API for IoT devices in the lab. This was followed by implementing this system for an Arduino-powered lab sign that could change colors depending on the requests it received through the internet. 

My second project involved working on the Computer Vision side of [InterLACE](https://ceeo.tufts.edu/research/projectsInterLACE.htm) (Interactive Learning and Collaboration Environment) which is a digital tool that teachers can use to enhance students' classroom experience by promoting collaboration and discussion. I wrote code to automatically and robustly extract different sections of worksheets based on given scanned images. This would act as a convenient classwork digitization tool which allowed teachers to go through the classwork section by section and share any interesting responses with the class. 

Using that tool, I explored the possibility of digitally parsing some of those sections into meaningful data. As a particular examples, a programmable worksheet was devised and implemented that young students could simply draw on to program robots:
<div class="hero hero--dark" style='height: 420px; background-image: url("/assets/images/experience/worksheet.PNG");'>
</div>